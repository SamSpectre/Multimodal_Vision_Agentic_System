# LangChain Vision & OCR Agentic System

Multimodal playground that showcases the **LangChain v1.0 + LangGraph** stack for vision-centric workflows. The repo contains two cooperating agents:

- A general-purpose **vision analysis agent** built with `create_agent()` and tool calling.
- A specialized **OCR/document agent** that chains EasyOCR/Tesseract tools inside a LangGraph-powered loop.

Together they demonstrate how the new LangChain patterns simplify sequential reasoning, tool routing, and memory for complex multimodal tasks.

---

## Highlights
- **LangGraph-native** execution: automatic state tracking, memory, checkpointing.
- **Tool-driven reasoning**: reusable toolset for image metadata, color analysis, quality checks, OCR, layout detection.
- **Multimodal messaging** via base64-encoded image payloads and GPT-4o-compatible prompts.
- **CLI demos** for both the generic vision assistant (`chat.py`) and OCR specialist (`ocr_agent.py`).

---

## Repository Layout
```
Project-4_Vision&Langchain/
â”œâ”€ config/
â”‚  â””â”€ settings.py             # Env loading, defaults, project paths
â”œâ”€ graph.py                   # Builds supervisor + specialists (multi-agent system)
â”œâ”€ chat.py                    # CLI entry point (uses supervisor agent)
â”œâ”€ ocr_agent.py               # Legacy standalone OCR CLI (optional)
â”œâ”€ MULTI_AGENT_GUIDE.md       # Notes on LangGraph multi-agent patterns
â”œâ”€ src/
â”‚  â”œâ”€ agents/                 # create_agent-based specialists + supervisor
â”‚  â”‚  â”œâ”€ supervisor.py        # Wraps specialists as tools for routing
â”‚  â”‚  â”œâ”€ vision_agent.py      # Image properties/colors/quality specialist
â”‚  â”‚  â”œâ”€ ocr_agent.py         # OCR/document specialist (EasyOCR/Tesseract)
â”‚  â”‚  â””â”€ qa_agent.py          # Context Q&A specialist (search & summarize tools)
â”‚  â”œâ”€ state/
â”‚  â”‚  â””â”€ graph_state.py       # TypedDict state definition for LangGraph
â”‚  â””â”€ tools/
â”‚     â”œâ”€ vision_utils.py      # Shared image helpers + multimodal message builder
â”‚     â”œâ”€ basic_vision_tools.py# Image metadata/color/quality diagnostics
â”‚     â””â”€ ocr_tools.py         # OCR/text-region/document-structure utilities
â”œâ”€ checkpoints/               # LangGraph memory snapshots (auto-created)
â”œâ”€ test_image.png, test_images/   # Sample assets for quick testing
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## Prerequisites
- **Python 3.10+** (LangChain v1.0 requirement)
- An OpenAI API key (and optional Anthropic key)
- Basic CV libraries (OpenCV, Pillow, NumPy) installed via `requirements.txt`

---

## Setup
1. (Optional) create & activate a virtual environment:
   ```bash
   python -m venv .venv
   .venv\Scripts\activate   # Windows
   source .venv/bin/activate # macOS/Linux
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Create a `.env` file in the project root:
   ```
   OPENAI_API_KEY=sk-...
   ANTHROPIC_API_KEY=optional
   ```
4. Validate configuration (prints detected keys, paths, models):
   ```bash
   python -m config.settings
   ```

---

## Usage

### Vision Agent CLI
```bash
python chat.py
```
- Drag/drop or type an image path, then ask questions like â€œanalyze this imageâ€ or â€œwhat are the dominant colors?â€
- The agent automatically runs the appropriate tool(s) and keeps conversation memory via LangGraph checkpoints.

### OCR Agent Demo
```bash
python ocr_agent.py
```
Commands inside the demo shell:
- `extract path/to/image.png` â€“ run full-text extraction.
- `analyze path/to/document.jpg` â€“ perform extraction plus layout + structure analysis.
- Any other prompt runs a general OCR Q&A with optional image input.

Both flows support multimodal prompts through `vision_utils.create_vision_message`.

---

## Extending the Graph
1. **Add tools** in `src/tools/`. Decorate with `@tool` to expose them to agents.
2. **Update state** in `src/state/graph_state.py` if you need to persist additional data (e.g., cached OCR payloads, routing flags).
3. **Modify `graph.py`** to:
   - Change the system prompt/routing guidelines.
   - Swap models via `config.settings`.
   - (Advanced) replace `create_agent()` with an explicit `StateGraph` to model custom nodes/edges.
4. **Compose agents** by importing tool lists (e.g., `basic_vision_tools`) or nested runnables inside new LangGraph nodes.

---

## Testing & Samples
- `test_image.png` plus `test_images/` hold quick-check assets for tool development.
- Each tool module includes a `__main__` block you can run directly for smoke tests.
- Add Pytest-based regression tests under `testing/` as you mature the pipeline.

---

## Roadmap Ideas
- Combine the vision and OCR graphs into a single routed LangGraph application.
- Persist checkpoints beyond memory (e.g., SQLite or S3) for longer conversations.
- Add structured outputs (Pydantic models) for document summaries.
- Integrate face/ID detection with MediaPipe (already listed in `requirements.txt`).

Pull requests welcomeâ€”happy building! ğŸš€
